{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec2e9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc079ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate ,MessagesPlaceholder\n",
    "from langchain_classic.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4faf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive splitter created\n",
      "number of splits: 181\n",
      "Token-based splitter created\n",
      "number of token-based splits: 76\n"
     ]
    }
   ],
   "source": [
    "pdf_paths=['./data/rag1.pdf', './data/rag2.pdf' ,'./data/rag3.pdf']\n",
    "all_docs=[]\n",
    "\n",
    "for path in pdf_paths:\n",
    "    loader=PyPDFLoader(path)\n",
    "    docs =loader.load()\n",
    "    all_docs.extend(docs)\n",
    "    \n",
    "recur_splitter=RecursiveCharacterTextSplitter(chunk_size=1000 ,chunk_overlap=200)\n",
    "splits=recur_splitter.split_documents(all_docs)\n",
    "\n",
    "token_splitter=TokenTextSplitter(chunk_size=1000, chunk_overlap=200 ,model_name='gpt-4o-mini')\n",
    "token_splits=token_splitter.split_documents(all_docs)\n",
    "\n",
    "\n",
    "\n",
    "model=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "recur_embeddings=model.embed_documents([doc.page_content for doc in splits])\n",
    "token_embeddings=model.embed_documents([doc.page_content for doc in token_splits])\n",
    "\n",
    "print('Recursive splitter created')\n",
    "print(f'number of splits: {len(splits)}')\n",
    "print('Token-based splitter created')\n",
    "print(f'number of token-based splits: {len(token_splits)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59cf28fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma,FAISS\n",
    "import shutil, os\n",
    "\n",
    "if os.path.exists(\"./chroma_recur_db\"):\n",
    "    shutil.rmtree(\"./chroma_recur_db\")\n",
    "    \n",
    "model=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "chroma_db=Chroma.from_documents(\n",
    "    documents=token_splits,\n",
    "    embedding=model,\n",
    "    collection_name='recur_chunks_collection',\n",
    "    persist_directory='./chroma_recur_db'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec9bca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Result 1 ----\n",
      "정천수\n",
      "102 지식경영연구 제25권 제3호\n",
      "을 거처 벡터 저장소에 저장된다(Microsoft, 2023). 이러\n",
      "한 RAG모델 기반 생성형 AI 서비스 구현 흐름은 <그림 \n",
      "1>과 같다(정천수, 2023d).\n",
      "2.1.2. RAG기반 Vector Store 유형\n",
      "RAG시스템을 구축하기 위해서는 지식이 저장되는 벡\n",
      "터 데이터베이스를 사용하게 되는데, 벡터 데이터베이스\n",
      "에 대한 일반적인 벡터 파이프라인은 인덱싱(Indexing), \n",
      "조회(Querying), 사후처리(Post Processing)와 같은 3단계\n",
      "를 거친다(Devtorium, 2023). 특히, RAG기반 벡터 저장소\n",
      "(Vector Store) 저장 유형은 <그림 2>와 같이 모든 소스 \n",
      "데이터를 사전에 Vector Store에 저장하는 경우와 질문 \n",
      "시 실시간으로 넣는 경우로 나눌 수 있다(정천수, 2023d).\n",
      "기업에서는 내부 지식을 Open LLM을 통해 서비스하\n",
      "게 되었을 때 보안 이슈 때문에 Local LLM을 사용하는 \n",
      "것이 중요한 이슈이다(정천수, 2023d). 이때 각 구간별로 \n",
      "가장 잘 처리할 수 있는 Local LLM을 여러 개 구성하여 \n",
      "사용하는 것이 효과적인데, <그림 2>에서 여러 개의 소\n",
      "<그림 1> RAG기반 생성형 AI 서비스 구현 흐름\n",
      "<그림 2> RAG기반 Vector Store 구성 유형 및 처리절차\n",
      "\n",
      "---- Result 2 ----\n",
      "정천수\n",
      "104 지식경영연구 제25권 제3호\n",
      "LangChain 또는 LlamaIndex 프레임워크는 이러한 전략\n",
      "을 구현할 수 있도록 각 항목별 라이브러리를 제공하고 \n",
      "있어 구현을 좀더 쉽게 할 수 있도록 하고 있다\n",
      ".\n",
      "2.2.2. Advanced RAG 유형 연구 및 개선 방향\n",
      "현재 연구되고 있는 대표적인 Advanced RAG 방안을 \n",
      "살펴보고, 각 유형의 강점과 약점을 분석하여 본 연구의 \n",
      "방향성을 제시하고자 한다. \n",
      "• Self-RAG: 이 방식은 생성된 답변을 다시 검색하여 \n",
      "관련 정보를 찾고, 이를 기반으로 답변을 개선하는 \n",
      "방법이다. 이를 통해 답변의 정확도와 유창성을 향\n",
      "상시킬 수 있다(Asai et al., 2023). 그러나 이 접근법\n",
      "은 검색된 정보의 품질에 크게 의존하며, 정보의 신\n",
      "뢰성을 평가하는 메커니즘이 부족하다는 단점이 있\n",
      "다\n",
      ". 또한, 반복적인 검색과 생성 과정에서 많은 리소\n",
      "스가 소모되므로, 이를 보완하기 위해 초기 검색 단\n",
      "계에서 더 정교한 필터링을 통해 검색 범위를 줄이\n",
      "고\n",
      ", 반복적인 처리를 최소화하는 최적화 기법이 필\n",
      "요하다.\n",
      "• Corrective RAG: 이 방식은 생성된 답변의 오류를 \n",
      "수정하기 위해 Corrective Agent를 사용하는 방법이\n",
      "다. Corrective Agent는 답변의 오류를 식별하고, 이\n",
      "를 수정하기 위한 정보를 검색한다. 이를 통해 답변\n",
      "의 신뢰성을 높일 수 있다(Yan et al., 2024). 그러나 \n",
      "초기 검색 결과가 부정확할 경우 이를 수정하는 과\n",
      "정이 명확하게 정의되지 않아\n",
      ", 추가적인 처리 시간\n",
      "이 필요하게 되며, 실제 적용 시 비효율성을 초래할 \n",
      "수 있다. 이를 보완하기 위해, 수정 프로세스를 구체\n",
      "화하고, 사전 검토 단계에서 잠재적인 오류를 미리 \n",
      "감지하여 이를 기반으로 답변을 생성하도록 유도하\n",
      "는 기법을 도입하면\n",
      ", 보다 효과적인 정보 검색 및 응\n",
      "답 생성이 가능해질 것이다.\n",
      "• Adaptive RAG: 이 방식은 질문의 유형에 따라 적절\n",
      "한 RAG 방식을 선택하여 적용하는 방법이다. 예를 \n",
      "들어, 사실적 질문에는 Self-RAG 방식을, 의견 질문\n",
      "에는 Corrective RAG 방식을 사용하는 등 질문 유형\n",
      "에 따라 최적의 방식을 선택함으로써 답변의 정확도\n",
      "를 높일 수 있다\n",
      "(Jeong et al., 2024). 그러나 현재 연\n",
      "구에서는 피드백 루프의 설계와 구현에 대한 구체적\n",
      "인 사례가 부족하다는 한계가 있다\n",
      ". 이를 개선하기 \n",
      "위해서는 질문 유형을 정확히 분류하는 것이 핵심 \n",
      "과제로\n",
      ", 분류의 정밀도를 높이기 위해 추가적인 학\n",
      "습 데이터를 활용하거나, 특정 도메인에 맞춘 분류 \n",
      "기법을 도입하는 방법을 고려할 수 있다.\n",
      "본 연구에서는 LangGraph와 같은 그래프 기반 기술 \n",
      "및 에이전트를 활용하여, 검색된 정보의 신뢰성을 효과\n",
      "적으로 평가하고, 사전에 잠재적인 오류를 미리 감지함\n",
      "으로써 응답 품질을 개선할 수 있는 방법을 제시하고자 \n",
      "한다\n",
      ".\n",
      "3. Advanced RAG 모델의 설계\n",
      "본 장에서는 기존 연구에서 제안된 다양한 Advanced \n",
      "RAG 방안들을 검토하고, 이를 기반으로 향상된 RAG 시스\n",
      "템을 설계한다. 특히, Self-RAG, Corrective RAG, Adaptive \n",
      "RAG 등의 방안들을 면밀히 분석하고, 이를 통해 얻은 개\n",
      "선점을 바탕으로 <그림 3>과 같이 구현 모델을 제시 한\n",
      "다. Agent RAG 시스템에 대한 구현은 Corrective RAG을 \n",
      "기본으로 하고 Self-RAG, Adaptive RAG를 참조하고 있다. \n",
      "일반적인 RAG 시스템을 향상시키기 위한 워크플로는 \n",
      "기존처럼 벡터 데이터베이스에서 문서 청크를 검색한 다\n",
      "음 \n",
      "LLM을 사용하여 검색된 각 문서 청크가 입력 질문과\n",
      "\n",
      "---- Result 3 ----\n",
      " 등 다양한 분야에 AI 시\n",
      "스템을 도입하고 있다. 하지만 이러한 시스템은 데이터 \n",
      "편향, 환각 문제, 실시간 데이터 처리의 어려움, 특정 분\n",
      "야에 대한 지식 부족 등의 한계를 가지고 있다. 특히, 사\n",
      "용자의 질문에 시스템이 답변을 찾지 못할 때 ‘죄송합니\n",
      "다, 모르겠습니다’라고 명확하게 답변하지 못하고, 관련 \n",
      "없는 정보를 마치 사실인 것처럼 답변을 제시하는 환각\n",
      "(Hallucination) 이 발생한다. 또한, 정확한 답변이 존재하\n",
      "지만 검색 결과의 순위가 낮아 답변에서 누락되는 경우\n",
      "도 발생한다\n",
      ". 특히, 기업의 사례를 보면, 금융 회사의 경\n",
      "우 과거 데이터로 인한 잘못된 예측으로 손실을 입고, 전\n",
      "자 상거래 기업은 실시간으로 재고 현황이나 배송 상태\n",
      "와 같은 정보를 반영하지 못해 고객 만족도가 크게 하락\n",
      "하는 문제가 발생한다\n",
      ". 또한 제조업체는 기술 지원을 위\n",
      "한 RAG 기반 시스템을 도입했으나, 시스템이 복잡한 기\n",
      "술 용어와 전문 지식을 제대로 이해하지 못해 고객들이 \n",
      "잘못된 정보를 제공받는 사례가 발생하게 된다\n",
      "(Scott et \n",
      "al., 2024; 정천수, 2024).\n",
      "이렇게 RAG모델의 효과는 고품질의 데이터베이스에 \n",
      "크게 의존하고 있기 때문에 데이터의 질이 모델 성능에 \n",
      "직접적인 영향을 미친다\n",
      "(김종철, 2024). 기존의 RAG 모\n",
      "델은 한 번에 지식을 적재한 후, 재작업 없이 답변을 생\n",
      "성함으로 정확도 저하 문제와 RAG구성 시점 이후의 실\n",
      "시간 데이터를 볼 수 없다. 이렇게 처음에 한번 벡터화한\n",
      "\n",
      "---- Result 4 ----\n",
      "지식 기반 QA개선을 위한 Advanced RAG 시스템 구현 방법\n",
      "Knowledge Management Research. Sep. 2024 101\n",
      "지식정보를 한 번의 답변 생성 과정을 거치기 때문에, 답\n",
      "변이 부정확할 경우에 지식정보를 새로운 정보로 재 작\n",
      "업하는 과정 없이 답변을 제공하여 답변의 정확도가 저\n",
      "하되는 경우가 발생될 수가 있다\n",
      ". 특히 복잡한 질문에 대\n",
      "한 답변을 생성할 때 이러한 문제가 생길 수 있으며, 기\n",
      "존 RAG 모델은 다양한 질문 유형에 효과적으로 대응하\n",
      "기 어려운 측면이 있었다. 잘못된 검색 전략으로 인해 관\n",
      "련 없는 문서가 질문에 답변하는 데 사용될 수 있고, 여\n",
      "전히 LLM에서 나타나는 환각을 겪거나 질문에 대답하\n",
      "지 못하는 경우가 발생되는 한계점을 가지고 있다(전준\n",
      "영 등, 2024)\n",
      "본 연구는 이러한 전통적인 QA 시스템에서 발생하는 \n",
      "정보 검색 및 환각 문제를 해결하기 위한 새로운 접근 \n",
      "방식을 제안하고자 한다\n",
      ". 특히, 실시간 데이터 접근과 문\n",
      "서 검증 프로세스를 통합하여 질문에 대한 정확한 답변\n",
      "을 제공할 수 있도록 하는 것을 목표로 한다\n",
      ". 실시간 데\n",
      "이터에 접근하고, 검색된 문서가 실제로 질문에 응답하\n",
      "는 데 관련이 있는지 확인한다. 이를 통해 RAG 시스템은 \n",
      "최근 이벤트와 실시간 데이터에 대한 질문에 답할 수 있\n",
      "고 환각에 덜 취약하게 향상된 \n",
      "RAG 시스템을 구현함으\n",
      "로써 생성형 AI 서비스의 품질 및 성능을 향상시키는 데 \n",
      "기여할 것이다. \n",
      "본 논문의 서론에서는 연구의 배경과 목적, 기존 RAG \n",
      "모델의 한계, 연구의 중요성 및 기여도, 논문의 구조에 \n",
      "대해 설명한다. 이론적 배경에서는 RAG 모델의 개요와 \n",
      "Advanced RAG 방안, 기존 연구 개선 유형 사례를 검토\n",
      "하고, Advanced RAG 모델의 설계에서는 Advanced RAG\n",
      "의 구성 흐름과 Agent RAG 구성, 기타 향상된 기능에 \n",
      "대해 다룬다. 시스템 구현에서는 LangGraph의 개요 및 \n",
      "적용 방법, 시스템 구현 과정 및 결과를 기술하고, 테스\n",
      "트에서는 구현된 코드의 개선된 결과값을 제시하고 있\n",
      "다\n",
      ". 마지막으로 결론에서는 연구 결과의 요약, 연구의 한\n",
      "계 및 향후 연구 방향에 대해 다루고 있다. \n",
      "2. 이론적 배경\n",
      "본 연구를 위해 RAG 모델과 관련된 자료에 대하여 최\n",
      "근 발표된 주요 연구 논문, 저널 기사 등을 면밀히 분석\n",
      "하고 조사하였다. 본 장에서는 RAG 모델의 개념부터 최\n",
      "신 연구 동향인 Advanced RAG까지 다루고 있다.\n",
      "2.1. RAG 모델의 개요\n",
      "RAG 모델은 질문에 대한 답변을 생성하기 위해 검색\n",
      "과 생성이라는 두 가지 과정을 결합한 모델이다(Lewis et \n",
      "al., 2020). 마치 사람이 궁금한 것을 해결하기 위해 책이\n",
      "나 인터넷을 검색하고, 찾은 정보를 바탕으로 답변을 구\n",
      "성하는 것과 비슷하다. 이 모델은 질문에 대한 답변을 생\n",
      "성하기 위해 먼저 관련 문서를 검색하고, 이를 바탕으로 \n",
      "답변을 생성한다. 이 과정은 질문에 대한 정확한 답변을 \n",
      "생성하는 데 도움이 된다. RAG 모델은 다양한 질문 유형\n",
      "에 대응할 수 있으며, 특정 도메인에 대한 지식이 부족할 \n",
      "경우에도 효과적으로 답변을 생성할 수 있다. 따라서 기\n",
      "존 생성 모델에 비해 정보의 정확성과 일관성을 높일 수 \n",
      "있다는 장점을 가지고 있다\n",
      ".\n",
      "RAG 모델은 아래와 같이 두 단계로 구성된다.\n",
      "• 검색 단계: 주어진 질문과 관련된 정보를 지식에서 \n",
      "검색 엔진을 통해 검색한다.\n",
      "• 생성 단계: 검색된 정보를 기반으로 답변을 생성한다. \n",
      "2.1.1. RAG 모델 구현 흐름\n",
      "RAG 모델은 텍스트 생성 작업을 수행하는 모델로 주\n",
      "어진 소스 데이터로부터 정보를 검색하고, 해당 정보를 \n",
      "활용하여 원하는 텍스트를 생성하는 과정을 수행한다(정\n",
      "천수, 2023d). RAG 사용을 위한 데이터처리 과정은 원본 \n",
      "데이터를 청크(Chunk)단위의 작은 조각으로 나누고 텍스\n",
      "트 데이터를 숫자인 벡터로 전환하는\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results=chroma_db.similarity_search('RAG의 장점' ,k=4)\n",
    "\n",
    "for i,doc in enumerate(results ,1):\n",
    "    print(f'---- Result {i} ----')\n",
    "    print(doc.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0449eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Result 1 ----\n",
      "정천수\n",
      "102 지식경영연구 제25권 제3호\n",
      "을 거처 벡터 저장소에 저장된다(Microsoft, 2023). 이러\n",
      "한 RAG모델 기반 생성형 AI 서비스 구현 흐름은 <그림 \n",
      "1>과 같다(정천수, 2023d).\n",
      "2.1.2. RAG기반 Vector Store 유형\n",
      "RAG시스템을 구축하기 위해서는 지식이 저장되는 벡\n",
      "터 데이터베이스를 사용하게 되는데, 벡터 데이터베이스\n",
      "에 대한 일반적인 벡터 파이프라인은 인덱싱(Indexing), \n",
      "조회(Querying), 사후처리(Post Processing)와 같은 3단계\n",
      "를 거친다(Devtorium, 2023). 특히, RAG기반 벡터 저장소\n",
      "(Vector Store) 저장 유형은 <그림 2>와 같이 모든 소스 \n",
      "데이터를 사전에 Vector Store에 저장하는 경우와 질문 \n",
      "시 실시간으로 넣는 경우로 나눌 수 있다(정천수, 2023d).\n",
      "기업에서는 내부 지식을 Open LLM을 통해 서비스하\n",
      "게 되었을 때 보안 이슈 때문에 Local LLM을 사용하는 \n",
      "것이 중요한 이슈이다(정천수, 2023d). 이때 각 구간별로 \n",
      "가장 잘 처리할 수 있는 Local LLM을 여러 개 구성하여 \n",
      "사용하는 것이 효과적인데, <그림 2>에서 여러 개의 소\n",
      "<그림 1> RAG기반 생성형 AI 서비스 구현 흐름\n",
      "<그림 2> RAG기반 Vector Store 구성 유형 및 처리절차\n",
      "\n",
      "---- Result 2 ----\n",
      "정천수\n",
      "104 지식경영연구 제25권 제3호\n",
      "LangChain 또는 LlamaIndex 프레임워크는 이러한 전략\n",
      "을 구현할 수 있도록 각 항목별 라이브러리를 제공하고 \n",
      "있어 구현을 좀더 쉽게 할 수 있도록 하고 있다\n",
      ".\n",
      "2.2.2. Advanced RAG 유형 연구 및 개선 방향\n",
      "현재 연구되고 있는 대표적인 Advanced RAG 방안을 \n",
      "살펴보고, 각 유형의 강점과 약점을 분석하여 본 연구의 \n",
      "방향성을 제시하고자 한다. \n",
      "• Self-RAG: 이 방식은 생성된 답변을 다시 검색하여 \n",
      "관련 정보를 찾고, 이를 기반으로 답변을 개선하는 \n",
      "방법이다. 이를 통해 답변의 정확도와 유창성을 향\n",
      "상시킬 수 있다(Asai et al., 2023). 그러나 이 접근법\n",
      "은 검색된 정보의 품질에 크게 의존하며, 정보의 신\n",
      "뢰성을 평가하는 메커니즘이 부족하다는 단점이 있\n",
      "다\n",
      ". 또한, 반복적인 검색과 생성 과정에서 많은 리소\n",
      "스가 소모되므로, 이를 보완하기 위해 초기 검색 단\n",
      "계에서 더 정교한 필터링을 통해 검색 범위를 줄이\n",
      "고\n",
      ", 반복적인 처리를 최소화하는 최적화 기법이 필\n",
      "요하다.\n",
      "• Corrective RAG: 이 방식은 생성된 답변의 오류를 \n",
      "수정하기 위해 Corrective Agent를 사용하는 방법이\n",
      "다. Corrective Agent는 답변의 오류를 식별하고, 이\n",
      "를 수정하기 위한 정보를 검색한다. 이를 통해 답변\n",
      "의 신뢰성을 높일 수 있다(Yan et al., 2024). 그러나 \n",
      "초기 검색 결과가 부정확할 경우 이를 수정하는 과\n",
      "정이 명확하게 정의되지 않아\n",
      ", 추가적인 처리 시간\n",
      "이 필요하게 되며, 실제 적용 시 비효율성을 초래할 \n",
      "수 있다. 이를 보완하기 위해, 수정 프로세스를 구체\n",
      "화하고, 사전 검토 단계에서 잠재적인 오류를 미리 \n",
      "감지하여 이를 기반으로 답변을 생성하도록 유도하\n",
      "는 기법을 도입하면\n",
      ", 보다 효과적인 정보 검색 및 응\n",
      "답 생성이 가능해질 것이다.\n",
      "• Adaptive RAG: 이 방식은 질문의 유형에 따라 적절\n",
      "한 RAG 방식을 선택하여 적용하는 방법이다. 예를 \n",
      "들어, 사실적 질문에는 Self-RAG 방식을, 의견 질문\n",
      "에는 Corrective RAG 방식을 사용하는 등 질문 유형\n",
      "에 따라 최적의 방식을 선택함으로써 답변의 정확도\n",
      "를 높일 수 있다\n",
      "(Jeong et al., 2024). 그러나 현재 연\n",
      "구에서는 피드백 루프의 설계와 구현에 대한 구체적\n",
      "인 사례가 부족하다는 한계가 있다\n",
      ". 이를 개선하기 \n",
      "위해서는 질문 유형을 정확히 분류하는 것이 핵심 \n",
      "과제로\n",
      ", 분류의 정밀도를 높이기 위해 추가적인 학\n",
      "습 데이터를 활용하거나, 특정 도메인에 맞춘 분류 \n",
      "기법을 도입하는 방법을 고려할 수 있다.\n",
      "본 연구에서는 LangGraph와 같은 그래프 기반 기술 \n",
      "및 에이전트를 활용하여, 검색된 정보의 신뢰성을 효과\n",
      "적으로 평가하고, 사전에 잠재적인 오류를 미리 감지함\n",
      "으로써 응답 품질을 개선할 수 있는 방법을 제시하고자 \n",
      "한다\n",
      ".\n",
      "3. Advanced RAG 모델의 설계\n",
      "본 장에서는 기존 연구에서 제안된 다양한 Advanced \n",
      "RAG 방안들을 검토하고, 이를 기반으로 향상된 RAG 시스\n",
      "템을 설계한다. 특히, Self-RAG, Corrective RAG, Adaptive \n",
      "RAG 등의 방안들을 면밀히 분석하고, 이를 통해 얻은 개\n",
      "선점을 바탕으로 <그림 3>과 같이 구현 모델을 제시 한\n",
      "다. Agent RAG 시스템에 대한 구현은 Corrective RAG을 \n",
      "기본으로 하고 Self-RAG, Adaptive RAG를 참조하고 있다. \n",
      "일반적인 RAG 시스템을 향상시키기 위한 워크플로는 \n",
      "기존처럼 벡터 데이터베이스에서 문서 청크를 검색한 다\n",
      "음 \n",
      "LLM을 사용하여 검색된 각 문서 청크가 입력 질문과\n",
      "\n",
      "---- Result 3 ----\n",
      " 등 다양한 분야에 AI 시\n",
      "스템을 도입하고 있다. 하지만 이러한 시스템은 데이터 \n",
      "편향, 환각 문제, 실시간 데이터 처리의 어려움, 특정 분\n",
      "야에 대한 지식 부족 등의 한계를 가지고 있다. 특히, 사\n",
      "용자의 질문에 시스템이 답변을 찾지 못할 때 ‘죄송합니\n",
      "다, 모르겠습니다’라고 명확하게 답변하지 못하고, 관련 \n",
      "없는 정보를 마치 사실인 것처럼 답변을 제시하는 환각\n",
      "(Hallucination) 이 발생한다. 또한, 정확한 답변이 존재하\n",
      "지만 검색 결과의 순위가 낮아 답변에서 누락되는 경우\n",
      "도 발생한다\n",
      ". 특히, 기업의 사례를 보면, 금융 회사의 경\n",
      "우 과거 데이터로 인한 잘못된 예측으로 손실을 입고, 전\n",
      "자 상거래 기업은 실시간으로 재고 현황이나 배송 상태\n",
      "와 같은 정보를 반영하지 못해 고객 만족도가 크게 하락\n",
      "하는 문제가 발생한다\n",
      ". 또한 제조업체는 기술 지원을 위\n",
      "한 RAG 기반 시스템을 도입했으나, 시스템이 복잡한 기\n",
      "술 용어와 전문 지식을 제대로 이해하지 못해 고객들이 \n",
      "잘못된 정보를 제공받는 사례가 발생하게 된다\n",
      "(Scott et \n",
      "al., 2024; 정천수, 2024).\n",
      "이렇게 RAG모델의 효과는 고품질의 데이터베이스에 \n",
      "크게 의존하고 있기 때문에 데이터의 질이 모델 성능에 \n",
      "직접적인 영향을 미친다\n",
      "(김종철, 2024). 기존의 RAG 모\n",
      "델은 한 번에 지식을 적재한 후, 재작업 없이 답변을 생\n",
      "성함으로 정확도 저하 문제와 RAG구성 시점 이후의 실\n",
      "시간 데이터를 볼 수 없다. 이렇게 처음에 한번 벡터화한\n",
      "\n",
      "---- Result 4 ----\n",
      "지식 기반 QA개선을 위한 Advanced RAG 시스템 구현 방법\n",
      "Knowledge Management Research. Sep. 2024 103\n",
      "스 데이터베이스에서 질문에 맞는 데이터베이스의 내용\n",
      "을 가져오는 DB Query를 생성하는데 특화된 LLM이나, \n",
      "특정지식에서 답변을 생성해줄 수 있는 도메인 특화된 \n",
      "LLM을 별도 구축할 수도 있다(정천수, 2023d).\n",
      "2.2. Advanced RAG 선행 연구\n",
      "2.2.1. RAG 성능을 향상시키는 방안\n",
      "RAG는 외부 저장소에서 검색하는 질문 처리 결과에 \n",
      "따라 프롬프트로 구성할 수 있는 데이터의 질에 영향을 \n",
      "받는다\n",
      ". 최근에는 이러한 일반적인 RAG의 한계를 해결\n",
      "하기 위해 다양한 Advanced RAG 방안들이 제시되고 있\n",
      "다. Advanced RAG는 기존 RAG 기법의 진화된 형태로, \n",
      "일반적인 RAG의 한계를 극복하고자 다양한 최적화 기\n",
      "법을 적용한 접근 방식이다. 최근 Yunfan G., et al. (2024) \n",
      "연구자들은 Advanced RAG를 검색 부분을 사전검색\n",
      "(Pre-Retrieval), 검색(Retrieval), 사후검색(Post-Retrieval) \n",
      "단계로 나누고 각 단계의 최적화를 실시해 정보의 정확\n",
      "성과 처리 효율을 크게 향상시키는 최적화 전략을 제시\n",
      "하고 있다\n",
      ". 또한 적절한 컨텍스트를 찾고, 적절한 응답을 \n",
      "생성하는 RAG 시스템의 품질 향상을 위한 전략으로 관\n",
      "련성(Relevance)에 따라 다시 랭킹(Re-Ranking)하여 정\n",
      "확도를 향상하는 방법(장동진, 2024) 등과 같은 다양한 개\n",
      "선 방법들이 <표 1>과 같이 제시되고 있다(Matt, 2023). \n",
      "<표 1> RAG 성능을 향상시키는 방법\n",
      "방 법 내 용\n",
      "Clean your data\n",
      "정보가 상충되거나 중복되는 데이터를 사용하는 경우 검색 시 올바른 컨텍스트를 찾는 데 어려움을 겪게 됨\n",
      "- Query에 대한 대답을 제대로 할 수 있도록 문서 자체를 잘 구축해 놔야 함, 모든 문서의 Summary를 만\n",
      "들어서 context 로 사용하는 것이 하나의 방법\n",
      "Explore different index \n",
      "types\n",
      "임베딩 기반의 유사도 검색 방식은 보통은 잘 동작하지만, 항상 Best 인 것은 아님\n",
      "- 전자상거래 에서 제품과 같은 특정 항목을 찾는 것에 대한 검색 은 키워드 검색 방식이 더 적합할 수 있\n",
      "고, 많은 시스템에서 하이브리드 방식을 사용(예; 특정 상품을 찾는 것은 키워드 기반 검색, 고객 정보, 지\n",
      "원에 대한 일반적인 것은 임베딩 기반의 검색)\n",
      "Experiment with your \n",
      "chunking approach\n",
      "Chunk 사이즈는 매우 중요하며, 일반적으로 작은 Chunk 일 때 더 성능이 좋으나 주변 정보의 부족한 문제\n",
      "가 발생함\n",
      "- 일반적으로 청크 크기가 작을수록 검색 시스템이 관련 컨텍스트 부분을 찾는 데 도움이 됨\n",
      "Play around with your \n",
      "base prompt\n",
      "할루시네이션을 줄이기 위해 주어진 컨텍스트정보에서 만 답변을 하도록 프롬프팅\n",
      "- 예: ‘당신은 고객지원 상담원이다. 사실에 근거한 정보만 제공하면서 최대한 도움을 주도록 설계되었다. \n",
      "사전학습 지식이 아닌 Given the context information를 바탕으로 질의에 답하세요.’\n",
      "Try meta-data filtering 청크에 관련성 있는 메타데이터 테그를 추가한 다음 이를 사용하여 결과를 처리(문서명, 페이지, 이메일, \n",
      "날짜 등)\n",
      "Use query routing\n",
      "하나 이상의 인덱스를 갖는 것이 유용한 경우가 많음. 그런 다음 쿼리가 들어올 때 적절한 인덱스로 라우팅\n",
      "- 예: 요약 질문을 처리하는 인덱스 하나, 지적 질문을 처리하는 인덱스 하나, 날짜에 민감한 질문에 적합한 \n",
      "인덱스가 있을 수 있음\n",
      "Look into reranking 랭킹 재지정을 사용하면 검색 시스템이 컨텍스트에 대한 유사한(Similarity) 상위 노드를 가져온 후, 다음 관\n",
      "련성(Relevance)에 따라 다시 랭킹하여 정확도 향상\n",
      "Consider query \n",
      "transformations\n",
      "첫 질문에 대한 관련 컨텍스트를 찾지 못하면, 질문 변경을 해서 다시 시도하여 답변 정확\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results=chroma_db.similarity_search('기존 RAG방식의 한계' ,k=4)\n",
    "\n",
    "for i,doc in enumerate(results ,1):\n",
    "    print(f'---- Result {i} ----')\n",
    "    print(doc.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8673868f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Result 1 ----\n",
      "0.58, it came\n",
      "at a computational cost of 11.71 seconds per\n",
      "query. In practice, the \"Hybrid\" or \"Original\"\n",
      "methods are recommended, as they maintain\n",
      "comparable performance with reduced latency.\n",
      "• Reranking Module: Reranking is critical to\n",
      "maintaining high-quality results, as demonstrated\n",
      "by a performance drop in its absence. Among\n",
      "DLM-based rerankers, monoT5 significantly out-\n",
      "performed monoBERT and RankLLaMA. This\n",
      "superiority can be attributed to monoT5’s larger\n",
      "parameter set and more extensive training data, as\n",
      "well as its encoder-decoder architecture, which\n",
      "provides enhanced natural language understand-\n",
      "ing compared to the decoder-only LLaMA model.\n",
      "MonoT5’s effectiveness in boosting the relevance\n",
      "of retrieved documents affirms the necessity of\n",
      "reranking in improving the quality of generated\n",
      "responses.\n",
      "• Repacking Module: The Reverse configuration\n",
      "exhibited superior performance, achieving an\n",
      "RAG score of 0.560. This highlights the impor-\n",
      "tance of positioning more relevant context closer\n",
      "to the query to yield optimal results.\n",
      "• Summarization Module: The Recomp extrac-\n",
      "tive summarization method demonstrated supe-\n",
      "rior performance over LongLLMLingua, an ab-\n",
      "17722\n",
      "\n",
      "---- Result 2 ----\n",
      ",\n",
      "rapid reranking can be done by summing the\n",
      "pre-calculated log probabilities corresponding to\n",
      "the query tokens for each document. TILDEv2\n",
      "further enhances efficiency and greatly reduces\n",
      "index size by indexing only document-present to-\n",
      "kens, using NCE loss, and document expansion.\n",
      "Our experiments were conducted on the MS\n",
      "MARCO Passage ranking dataset (Bajaj et al.,\n",
      "2016). We followed and made modifications to the\n",
      "implementation provided by PyGaggle (Nogueira\n",
      "et al., 2020) and TILDE, using the models monoT5,\n",
      "monoBERT, RankLLaMA and TILDEv2. Rerank-\n",
      "ing results are shown in Table 10. We recommend\n",
      "monoT5 as a comprehensive method balancing\n",
      "performance and efficiency. RankLLaMA is suit-\n",
      "able for achieving the best performance, while\n",
      "TILDEv2 is ideal for the quickest experience on a\n",
      "fixed collection. Details on the experimental setup\n",
      "and results are presented in Appendix A.4.\n",
      "3.6 Document Repacking\n",
      "The performance of subsequent processes, such\n",
      "as LLM response generation, may be affected by\n",
      "the order documents are provided. To address this\n",
      "issue, we incorporate a compact repacking mod-\n",
      "ule into the workflow after reranking, featuring\n",
      "three repacking methods: “forward”, “reverse”\n",
      "17720\n",
      "\n",
      "---- Result 3 ----\n",
      "Method Commonsense Fact Check ODQA Multihop Med RAG Avg.\n",
      "Acc Acc EM F1 Score EM F1 Score Acc Score Score F1 Latency\n",
      "without retrieval\n",
      "+ baseline 0.537 0.560 0.373 0.413 0.428 0.167 0.173 0.182 0.360 - 0.351 0.292 1.27\n",
      "classification module , Hybrid with HyDE, monoT5, sides, Recomp\n",
      "w/o classification 0.719 0.505 0.391 0.450 0.478 0.212 0.255 0.254 0.528 0.540 0.422 0.353 16.58\n",
      "+ classification 0.727 0.595 0.393 0.450 0.479 0.207 0.257 0.254 0.460 0.580 0.443 0.353 11.71\n",
      "with classification, retrieval module , monoT5, sides, Recomp\n",
      "+ HyDE 0.718 0.595 0.320 0.373 0.380 0.170 0.213 0.222 0.400 0.545 0.398 0.293 11.58\n",
      "+ Original 0.721 0.585 0.300 0.350 0.363 0.153 0.197 0.206 0.390 0.486 0.383 0.273 1.44\n",
      "+ Hybrid 0.718 0.595 0.347 0.397 0.418 0.190 0.240 0.233 0.750 0.498 0.429 0.318 1.45\n",
      "+ Hybrid + HyDE 0.727 0.595 0.393 0.450 0.479 0.207 0.257 0.254 0.460 0.580 0.443 0.353 11.71\n",
      "with classification, Hybrid with HyDE, reranking module , sides, Recomp\n",
      "w/o reranking 0.720 0.591 0.365 0.429 0.435 0.211 0.260 0.253 0.512 0.530 0.430 0.334 10.31\n",
      "+ monoT5 0.727 0.595 0.393 0.450 0.479 0.207 0.257 0.253 0.460 0.580 0.443 0.353 11.71\n",
      "+ monoBERT 0.723 0.593 0.383 0.443 0.463 0.217 0.259 0.253 0.482 0.551 0.438 0.351 11.65\n",
      "+ RankLLaMA 0.723 0.597 0.382 0.443 0.459 0.197 0.240 0.237 0.454 0.558 0.431 0.342 13.51\n",
      "+ TILDEv2 0.725 0.588 0.394 0.456 0.473 0.209 0.255 0.249 0.486 0.536 0.440 0.355 11.26\n",
      "with classification, Hybrid with HyDE, monoT5, repacking module , Recomp\n",
      "+ sides 0.727 0.595 0.393 0.450 0.479 0.207 0.257 0.253 0.460 0.580 0.443 0.353 11.71\n",
      "+ forward 0.722 0.599 0.379 0.437 0.458 0.215 0.260 0.254 0.472 0.542 0.437 0.349 11.68\n",
      "+ reverse 0.728 0.592 0.387 0.445 0.473 0.219 0.263 0.260 0.532 0.560 0.446 0.354 11.70\n",
      "with classification, Hybrid with HyDE, monoT5, reverse, summarization module\n",
      "w/o summarization 0.729 0.591 0.402 0.457 0.468 0.205 0.252 0.245 0\n",
      "\n",
      "---- Result 4 ----\n",
      "\n",
      "ChatGPT-4o, Claude, Llama3 등을 사용하지 않\n",
      "았다. 이를 활용하여 RAG 모델을 구축한다면 높\n",
      "은 성능을 보일 수 있을 것으로 기대한다.\n",
      "마지막으로, LLM 활용으로 인한 비용 문제이\n",
      "다. ChatGPT-4와 LLM은 좋은 성능을 보이지\n",
      "만, 토큰 사용 비용이 커진다. 비용 부담을 줄이\n",
      "려면, Llama3과 같이 오픈된 모델을 활용하거나,\n",
      "작은 크기의 언어 모델을 활용해야 한다. 기존\n",
      "LLM보다는 성능이 떨어질 수 있지만, 효과적인\n",
      "결과를 얻기 위하여 파인튜닝과 같은 방법을 고\n",
      "려해 볼 수 있다. 래피드마이너 Generative\n",
      "Models는 LLM을 파인튜닝할 수 있는 오퍼레이\n",
      "터도 제공한다. 래피드마이너를 통하여 코드 없\n",
      "이 LLM을 파인튜닝하여 RAG 모델 성능을 개선\n",
      "하는 것을 향후 과제로 제안할 수 있다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results=chroma_db.similarity_search('monoT5와 RankLLaMA 중 성능이 더 좋은 것은?' ,k=4)\n",
    "\n",
    "for i,doc in enumerate(results ,1):\n",
    "    print(f'---- Result {i} ----')\n",
    "    print(doc.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51987630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asguug\\AppData\\Local\\Temp\\ipykernel_173552\\2680780438.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  chroma_db=Chroma(\n"
     ]
    }
   ],
   "source": [
    "#실행\n",
    "model=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "chroma_db=Chroma(\n",
    "    collection_name='recur_chunks_collection',\n",
    "    persist_directory='./chroma_recur_db',\n",
    "    embedding_function=model\n",
    ")\n",
    "query='monoT5와 RankLLaMA 중 성능이 더 좋은 것은?'\n",
    "retriever= chroma_db.as_retriever(search_kwargs={'k':2})\n",
    "docs=retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6e8603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실행\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    '''다음 컨텍스트만 사용해 질문에 답하세요.\n",
    "    컨텍스트:{context}\n",
    "    질문:{question}\n",
    "    '''\n",
    ")\n",
    "\n",
    "llm=ChatOpenAI(model_name='gpt-4o-mini' ,temperature=0)\n",
    "llm_chain=prompt |llm\n",
    "# result= llm_chain.invoke({'context':docs ,'question':query})\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019828e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "재작성한 쿼리: \"Comparison of performance between monoT5 and RankLLaMA**\"\n",
      "monoT5가 RankLLaMA보다 성능이 더 좋습니다. monoT5는 더 큰 파라미터 집합과 더 광범위한 훈련 데이터를 가지고 있으며, 자연어 이해에서 우수한 성능을 보여줍니다. Reranking 모듈에서 monoT5는 monoBERT와 RankLLaMA보다 뛰어난 성능을 발휘했습니다.\n"
     ]
    }
   ],
   "source": [
    "rewrite_prompt=ChatPromptTemplate.from_template(\n",
    "    '''\n",
    "    검색 엔진이 주어진 질문에 답할 수 있도록 더 나은 영문 검색어를 제공하세요. 쿼리는 \\'**\\'로 끝내세요.\n",
    "    질문:{x}\n",
    "    답변:\n",
    "    '''\n",
    ")\n",
    "\n",
    "def parse_rewriter_output(message):\n",
    "    return message.content.strip('\\'').strip('**')\n",
    "rewriter=rewrite_prompt | llm |parse_rewriter_output\n",
    "\n",
    "@chain\n",
    "def qa_rrr(input):\n",
    "    new_query=rewriter.invoke(input)\n",
    "    print('재작성한 쿼리:' ,new_query)\n",
    "    docs=retriever.invoke(new_query)\n",
    "    formatted =prompt.invoke({'context':docs ,'question':input})\n",
    "    answer =llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "result =qa_rrr.invoke(query)\n",
    "print(result.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d9f7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실행\n",
    "#다중 쿼리 검색\n",
    "perspective_prompt= ChatPromptTemplate.from_template(\n",
    "    '''당신은 AI 언어 모델 어시스턴트입니다.\n",
    "    주어진 사용자 질문의 다섯 가지 버전을 생성하여 벡터 데이터베이스에서 관련 문서를 검색하세요.\n",
    "    사용자 질문에 대한 다양한 관점을 생성함으로써 사용자가 거리 기반 유사도 검색의 한계를 극복할 수 있도록 돕는 것이 \n",
    "    목표입니다.이러한 대체 질문을 개행으로 구분하여 제공하세요.\n",
    "    원래질문:{question}'''\n",
    ")\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split('\\n')\n",
    "\n",
    "query_gen=perspective_prompt |llm |parse_queries_output\n",
    "\n",
    "def get_unique_union(document_lists):\n",
    "    deduped_docs={doc.page_content:doc for sublist in document_lists for doc in sublist}\n",
    "    return list(deduped_docs.values())\n",
    "\n",
    "retrieval_chain= query_gen | retriever.batch | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2683c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다중 쿼리 검색\n",
      "\n",
      "답변: 문서에 따르면, monoT5는 monoBERT와 RankLLaMA보다 성능이 우수하다고 언급되어 있습니다. 그러나 RankLLaMA는 최고의 성능을 달성하는 데 적합하다고도 설명되어 있습니다. 따라서 두 모델의 성능은 각각의 목적에 따라 다를 수 있습니다.\n",
      "\n",
      "[근거 문서 정보]\n",
      "1. 파일명:./data/rag1.pdf ,페이지/섹션:6\n",
      "2. 파일명:./data/rag1.pdf ,페이지/섹션:7\n",
      "3. 파일명:./data/rag1.pdf ,페이지/섹션:4\n",
      "4. 파일명:./data/rag3.pdf ,페이지/섹션:7\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    '''\n",
    "    당신은 주어진 기술 문서에 기반해서만 답변하는 AI 어시스턴트입니다.\n",
    "    반드시 제공된 문서 내용을 기반으로 답변하고, 문서에 없는 내용은 ‘문서에 정보가 없습니다’라고 답하세요.\n",
    "    컨텍스트:{context}\n",
    "    질문:{question}\n",
    "    '''\n",
    ")\n",
    "query='monoT5와 RankLLaMA 중 성능이 더 좋은 것은?'\n",
    "\n",
    "@chain\n",
    "def multi_query_qa(input):\n",
    "    docs=retrieval_chain.invoke(input)\n",
    "    formatted=prompt.invoke({'context':docs ,'question':input})\n",
    "    answer=llm.invoke(formatted)\n",
    "    return {\"answer\" :answer ,'docs':docs}\n",
    "\n",
    "print('다중 쿼리 검색\\n')\n",
    "result= multi_query_qa.invoke(query)\n",
    "print(f\"답변: {result['answer'].content}\\n\")\n",
    "\n",
    "print(\"[근거 문서 정보]\")\n",
    "for i,doc in enumerate(result['docs'] ,1):\n",
    "    source =doc.metadata.get('source' ,'파일 정보 없음')\n",
    "    page=doc.metadata.get('page' ,'페이지 정보 없음')\n",
    "    print(f\"{i}. 파일명:{source} ,페이지/섹션:{page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5443266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실행\n",
    "#RAG 융합\n",
    "def reciprocal_rank_fusion(results:list[list] ,k=60):\n",
    "    '''여러 순위 문서 목록에 대해 상호 순위 융합 및 RRF 공식에 사용되는 선택적 매개변수 k입니다.\n",
    "    '''\n",
    "    fused_scores={}\n",
    "    documents={}\n",
    "    for docs in results:\n",
    "        for rank ,doc in enumerate(docs):\n",
    "            doc_str=doc.page_content\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str]=0\n",
    "                documents[doc_str]=doc\n",
    "            fused_scores[doc_str]+=1/(rank+k)\n",
    "            \n",
    "    reranked_doc_strs=sorted(fused_scores ,key=lambda d:fused_scores[d] ,reverse=True)\n",
    "    return [documents[doc_str] for doc_str in reranked_doc_strs]\n",
    "\n",
    "retrieval_chain=query_gen | retriever.batch |reciprocal_rank_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "106749c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실행\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    '''\n",
    "    당신은 주어진 기술 문서에 기반해서만 답변하는 AI 어시스턴트입니다.\n",
    "    반드시 제공된 문서 내용을 기반으로 답변하고, 문서에 없는 내용은 ‘문서에 정보가 없습니다’라고 답하세요.\n",
    "    컨텍스트:{context}\n",
    "    질문:{question}\n",
    "    '''\n",
    ")\n",
    "query='monoT5와 RankLLaMA 중 성능이 더 좋은 것은?'\n",
    "\n",
    "@chain\n",
    "def rag_fusion(input):\n",
    "    docs=retrieval_chain.invoke(input)\n",
    "    formatted=prompt.invoke({'context':docs ,'question':input})\n",
    "    answer=llm.invoke(formatted)\n",
    "    return {\"answer\" :answer ,'docs':docs}\n",
    "\n",
    "# print('RAG 융합 실행\\n')\n",
    "# result= rag_fusion.invoke(query)\n",
    "# print(f\"답변: {result['answer'].content}\\n\")\n",
    "\n",
    "# print(\"[근거 문서 정보]\")\n",
    "# for i,doc in enumerate(result['docs'] ,1):\n",
    "#     source =doc.metadata.get('source' ,'파일 정보 없음')\n",
    "#     page=doc.metadata.get('page' ,'페이지 정보 없음')\n",
    "#     print(f\"{i}. 파일명:{source} ,페이지/섹션:{page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c126915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하이브리드 서치는 두 가지 검색 방식을 사용합니다: 희소 검색(sparse retrieval)과 밀집 검색(dense retrieval)입니다. 희소 검색에는 BM25 알고리즘이 사용되며, 밀집 검색에는 Contriever라는 비지도 대조 텍스트 인코더가 사용됩니다.\n",
      "--------------------------------------\n",
      "RapidMiner는 셀프서비스 분석을 지원하는 예측적 데이터 분석 플랫폼으로, 완전 GUI 방식으로 데이터 로딩부터 모델 적용까지 시각적으로 작업 흐름을 설계할 수 있습니다. 비전문가도 쉽게 사용할 수 있도록 설계되어 있어 프로그래밍을 알지 못하더라도 RAG 구축 과정을 직관적으로 이해하고 구출할 수 있습니다.\n",
      "--------------------------------------\n",
      "문서에 정보가 없습니다.\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#실행\n",
    "while True:\n",
    "    question=input(\"질문을 입력하세요 (종료:exit) :\")\n",
    "    if question.lower() in ['exit','quit']:\n",
    "        break\n",
    "    answer = rag_fusion.invoke(question)\n",
    "    print(answer['answer'].content)\n",
    "    print('--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f5b82f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG 융합 (메모리 적용) 실행\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "memory=ConversationBufferWindowMemory(k=3 ,return_messages=True ,memory_key='chat_history')\n",
    "\n",
    "condense_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"이전 대화 내역과 최신 사용자 질문이 주어졌을 때, \n",
    "    이 질문이 이전 대화 맥락을 필요로 한다면 질문과 이전 대화 목록을 같이 넘겨주세요. \n",
    "    대화 내역: {chat_history}\n",
    "    질문: {question}\n",
    "    독립적인 질문:\"\"\"\n",
    ")\n",
    "condense_chain = condense_prompt | llm | StrOutputParser()\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\" ,'''\n",
    "     당신은 주어진 기술 문서에 기반해서만 답변하는 AI 어시스턴트입니다.\n",
    "    반드시 제공된 문서 내용을 기반으로 답변하고, 문서에 없는 내용은 ‘문서에 정보가 없습니다’라고 답하세요.\n",
    "    이전 대화 기록을 참고하여 자연스럽게 답변하세요.\n",
    "    '''),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    (\"human\" ,\"컨텍스트:{context}\\n질문:{question}\")\n",
    "])\n",
    "\n",
    "@chain\n",
    "def rag_fusion_with_memory(input):\n",
    "    history=memory.load_memory_variables({})['chat_history']\n",
    "    \n",
    "    if history:\n",
    "        search_query=condense_chain.invoke({\"chat_history\":history ,\"question\":input})\n",
    "    else:\n",
    "        search_query=input\n",
    "    print(f\"검색용 커리: {search_query}\")\n",
    "    \n",
    "    docs=retrieval_chain.invoke(search_query)\n",
    "    \n",
    "    formatted=prompt.invoke({\n",
    "        'context':docs,\n",
    "        'question':input,\n",
    "        'chat_history':history\n",
    "    })\n",
    "    answer=llm.invoke(formatted)\n",
    "    memory.save_context(\n",
    "        {'question':input},\n",
    "        {'output':answer.content}\n",
    "    )\n",
    "    return {\"answer\":answer ,'docs':docs}\n",
    "\n",
    "print('RAG 융합 (메모리 적용) 실행\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a28d400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색용 커리: 'Hybrid Search'는 어떤 두 가지 검색 방식을 결합한 것인가요? \n",
      "'Hybrid Search'는 sparse retrieval 방식인 BM25와 dense retrieval 방식인 Original embedding을 결합한 것입니다.\n",
      "검색용 커리: 이 질문은 이전 대화 맥락을 필요로 합니다. 따라서 질문과 이전 대화 목록을 함께 제공합니다.\n",
      "\n",
      "대화 내역:\n",
      "1. Human: \"'Hybrid Search'는 어떤 두 가지 검색 방식을 결합한 것인가요?\"\n",
      "2. AI: \"'Hybrid Search'는 sparse retrieval 방식인 BM25와 dense retrieval 방식인 Original embedding을 결합한 것입니다.\"\n",
      "\n",
      "질문: 이 방식과 HyDE를 함께 사용했을 때의 장단점은 무엇인가요?\n",
      "Hybrid Search와 HyDE를 함께 사용했을 때의 장점은 성능 향상입니다. HyDE는 가상의 문서를 생성하여 검색 성능을 크게 향상시킬 수 있으며, Hybrid Search는 BM25와 Original embedding을 결합하여 효율적인 검색을 제공합니다. 이 조합은 높은 성능을 유지하면서도 상대적으로 낮은 지연 시간을 달성할 수 있습니다.\n",
      "\n",
      "단점으로는, 가상의 문서를 여러 개 연결하는 경우 검색 성능이 향상될 수 있지만, 지연 시간이 증가하는 트레이드오프가 존재합니다. 또한, 가상의 문서 수를 무작정 늘리는 것은 큰 이점을 주지 않으며, 오히려 지연 시간을 상당히 증가시킬 수 있습니다. 따라서, HyDE와 Hybrid Search를 사용할 때는 적절한 가상의 문서 수를 선택하는 것이 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "#실행\n",
    "while True:\n",
    "    question = input(\"질문을 입력하세요 (종료: exit): \")\n",
    "    if question.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    answer = rag_fusion_with_memory.invoke(question)\n",
    "    print(answer['answer'].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
